---
title: "Generating phenotypes from 1KGP"
output: slidy_presentation
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
if (!require(pacman))
  install.packages("pacman")
pacman::p_load(tidyverse, pcgen2, tictoc, cowplot,flow, plantuml, dscrutils, 
               MegaLMM)
devtools::load_all(path = "/home/azza/github_repos/pcgen2/")

```


## Working with synthetic data:

```{r  warning=FALSE, message=FALSE}

data.syn <- readRDS("/home/azza/github_repos/pcgen2/results//mouse/calculate_K_1_er_model_1.rds")

#plot.nw(data.syn$true_dag$dag, title = "True DAG")

learnt.dag <- pcRes(data.syn$data, K = data.syn$K,
                    cov.method = "uni", m.max = Inf, verbose = F, alpha = 1e-3)
par(mfrow=c(1,3))
plot.nw(data.syn$true_dag$gdag, "True Genetic and traits network", fontsize = 10)
plot.nw(data.syn$true_dag$dag, "True traits-only network", fontsize = 10)
plot.nw(learnt.dag, "pcRes learnt network", fontsize = 15)


eval.nw(runs = 5, data.syn$true_dag$dag, suffStat = data.syn$data,
        K = data.syn$K, cov.method = "uni", m.max = Inf)

```

## conclusions:

These plots show:

- The genotypes effects are not learnt by `pcRes`, this follows from using the `pc` algorithm that does not account for confounding. It means we expect a bit higher level of errors in learning the network, and we see this when comparing with the *traits-only network* 

- Yet, we do see those traits with direct genetic effect to be related to each other in our learnt network. 
- We also see in the learnt network that the edges between traits with no direct effects to have been largely learnt (albiet with the wrong orientation at times).

## Idea 1:

  + Consider a confounding-friendly algorithm (something other than pc)

    ```{r echo = FALSE, eval=T}
    methods <- tribble(~Method, ~Abbreviation, ~Remarks,
                       "Fast Causal Inferernce", "fci", "computationally unfeasible for large graphs",
                       "Really Fast Causal Inference", "rfci", "much faster, output slightly less informative than fci (wrt CI information), but correct" )
    
    methods 

    

    ```

  + Both these methods are consistent in sparse dimensional settings
  + Both are based on the pc algorith, and similarly, have order-independent variants
  + With the data at hand

```{r}

learnt.dag.fci <- fciRes(data.syn$data, K = data.syn$K, cov.method = "uni", verbose = F)

par(mfrow = c(1, 3))
plot.nw(data.syn$true_dag$gdag, "True Genetic and traits network", fontsize = 10)
plot.nw(learnt.dag.fci, "fciRes learnt network", fontsize = 15)
plot.nw(learnt.dag, "pcRes learnt network", fontsize = 15)

eval.nw(runs = 5, data.syn$true_dag$dag, suffStat = data.syn$data,
        K = data.syn$K, cov.method = "uni", m.max = Inf, method = "fci")

# eval.nw(runs = 5, data.syn$true_dag$gdag, suffStat = data.syn$data,
#         K = data.syn$K, cov.method = "uni", m.max = Inf)

```

  + What's the right way to evaluate in this case? I'm caluclating SHD, FPR, .. etc, by converting the adjacency matrix into a graph object first, ignoring all edge info (similar to how it is done with pcRes evals). In essence, this ignores different possible values on the adjacency matrix: 
  
  * PAG (column index)
    - 0: No edge
    - 1: Circle
    - 2: Arrowhead
    - 3: Tail
  * CPDAG (row index)
     - 0: No edge or tail
     - 1: Arrowhead
    
```{r, eval=FALSE, echo = T}
PAG 
  amat[a,b] = 2  and  amat[b,a] = 3   implies   a --> b.
  amat[a,b] = 3  and  amat[b,a] = 2   implies   a <-- b.
  amat[a,b] = 2  and  amat[b,a] = 2   implies   a <-> b.
  amat[a,b] = 1  and  amat[b,a] = 3   implies   a --o b.
  amat[a,b] = 0  and  amat[b,a] = 0   implies   a     b.
  
CPDAG:
  amat[a,b] = 0  and  amat[b,a] = 1   implies a --> b.
  amat[a,b] = 1  and  amat[b,a] = 0   implies a <-- b.
  amat[a,b] = 0  and  amat[b,a] = 0   implies a     b.
  amat[a,b] = 1  and  amat[b,a] = 1   implies a --- b.
```

## idea 2:

  + learn via pcgen only
  
    - initially, we didn't take this route, because the mouse dataset contains covariates, and it is easier to work with them away from genotype effects.
    
  
```{r eval=F}  

pcgen.nw <- pcgen(suffStat = data.syn$data, K = data.syn$K,use.manova = F )

```

## idea 3: 
  + finish learning with `pcgen`
  Continued in a new Rmd
```{r}  

```
  
## idea 4: 
  + compare with results from learning a MVLMM!
  


